using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.Net;
using System.Text.RegularExpressions;
using System.Security.Cryptography.X509Certificates;
using System.Text;
using System.Linq;

//This is a high level object model for designing a recurrent neural network optimized via stochastic gradient descent
//It currently supports three different activation functions, support for passing in both learning rate and momentum factor, as well as multiple hidden layers
//This class was not designed for performance, rather to present an intuitive means for interacting with an otherwise complex subject
//This architecture has successfully performed color and object recognition

public class NeuralNetwork
{
    //Helper classes
    public class Dendrite
    {
        public double Weight;

        public Dendrite(double InputVal)
        {
            this.Weight = InputVal;
        }

        public Dendrite()
        {
        }
    }

    public class Bias
    {
        public double Weight = 1;
        public double Value = 1;
    }

    public class Neuron
    {
        public int Layer;
        public double Value;
        public double NetInput;
        //public double Bias;
        public Dendrite[] Dendrites;
        public double Error = 0;
        public double Softmax = 0;
        public int Index = 0;
    }

    public class Layer
    {
        public Neuron[] Neurons;
        public int LayerID;
        public double TotalLayerError = 0;
        public Bias BiasNeuron;

        public Layer(int LayerNum)
        {
            this.LayerID = LayerNum;
        }
    }



    //Properties
    public List<int> LayerCounts;
    public Layer[] Layers;
    public List<Neuron> Neurons = new List<Neuron>();
    public double LearningRate;
    public Random Rnd;
    public double Momentum = 0.7;
    public int ActivationFunctionID = 3;
    private double LastTotalError = 1;
    private double CurrentTotalError = 0;
    public double IOScaleFactor = 1;
    public double ParameterScaleFactor = 1;
    private double GradientClipFactor = 1000000000.0f;
    public double M = 0;
    public double V = 0;
    public int TrainingIteration = 0;
    public double ReLUConst = 0.00001;
    private double InitialLearningRate;
    public int NormalizationMethod = 0;

    //Methods
    public void InitializeNeuralNetwork(List<int> LayerObj, double Rate, int FunctionID,double IOScalingFactor,double ParameterScalingFactor)
    {
        int X = 0;
        int Y = 0;
        int Z = 0;
        double TempVal = 0;
        double TempVal2 = 0;
        double WeightRange = 0.999f;
        Neuron TempNeuron;
        Dendrite TempDendrite;
        Layer TempLayer;
        List<Dendrite> TempListDendrite;
        List<Neuron> TempNeuronList;
        List<Layer> TempLayerList;

        //Initialize properties
        this.LayerCounts = LayerObj;
        this.LearningRate = Rate;
        this.Rnd = new Random();
        this.ActivationFunctionID = FunctionID;
        this.IOScaleFactor = IOScalingFactor;
        this.ParameterScaleFactor = ParameterScalingFactor;
        this.InitialLearningRate = LearningRate;

        this.Layers = new Layer[1];
        TempLayerList = new List<Layer>();

        //For each layer....
        while (X < LayerObj.Count)
        {
            TempNeuronList = new List<Neuron>();
            TempLayer = new Layer(X);
            //TempLayer.Bias = (double)(ParameterScalingFactor * (this.Rnd.NextDouble() * 0.9));

            TempLayer.BiasNeuron = new Bias();
            //TempLayer.BiasNeuron.Weight = (double)(this.Rnd.NextDouble() * WeightRange);
            TempLayer.BiasNeuron.Weight = 1.0f;

            TempLayer.BiasNeuron.Value = 1.0f;
            

            Y = 0;

            //For each neuron....
            while (Y < LayerObj[X])
            {
                TempNeuron = new Neuron();
                TempNeuron.Layer = X;
                TempNeuron.Index = Y;

                //NOTE: If your input values into the network are large, in scalar terms (>255), you should keep these pretty small to prevent overflow
                //0.9 can be replaced with any value where 0 < X < 1
                //if (X == 0)
                //{
                //    //TempNeuron.Bias = (double)(ParameterScalingFactor * (this.Rnd.NextDouble() * 0.9));
                //    //TempNeuron.Bias = 1;
                //}
                //else
                //{
                //    //TempNeuron.Bias = (double)(ParameterScalingFactor * (this.Rnd.NextDouble() * 0.9));
                //    //TempNeuron.Bias = 1;
                //}

                TempVal = 0;

                Z = 0;
                if (X == 0)
                {
                    TempVal = LayerObj[0];
                }
                else
                {
                    TempVal = LayerObj[X - 1];
                }

                TempListDendrite = new List<Dendrite>();

                //For each dendrite....
                while (Z < TempVal)
                {
                    TempDendrite = new Dendrite();

                    //NOTE: If your input values into the network are large, in scalar terms (>255), you should keep these pretty small to prevent overflow
                    //0.9 can be replaced with any value where 0 < X < 1
                    if (X == 0)
                    {
                        TempVal2 = (double)(ParameterScalingFactor * (this.Rnd.NextDouble() * 0.9));
                        //TempVal2 = (double)(ParameterScalingFactor * (this.Rnd.NextDouble() * WeightRange) * (2.0f / LayerObj[X]));
                        TempDendrite.Weight = TempVal2;
                    }
                    else
                    {
                        TempVal2 = (double)(ParameterScalingFactor * (this.Rnd.NextDouble() * 0.9));
                        //TempVal2 = (double)(ParameterScalingFactor * (this.Rnd.NextDouble() * WeightRange) * (2.0f / LayerObj[X - 1]));
                        TempDendrite.Weight = TempVal2;
                    }

                    if(double.IsNaN(TempVal2))
                    {
                        break;
                    }

                    if(TempDendrite.Weight == 0 || double.IsNaN(TempDendrite.Weight))
                    {
                        break;
                    }

                    TempListDendrite.Add(TempDendrite);

                    Z++;
                }

                //Add list of generated dendrites
                TempNeuron.Dendrites = TempListDendrite.ToArray();
                TempNeuronList.Add(TempNeuron);

                Y++;
            }

            //Add list of generated neurons
            TempLayer.Neurons = TempNeuronList.ToArray();
            TempLayerList.Add(TempLayer);


            X++;
        }

        //Add list of generated layers
        this.Layers = TempLayerList.ToArray();
    }

    public void FeedForward(List<double> Inputs,List<int> ActivationFunctionIDs,bool NormalizeOutput = false)
    {
        int X = 1;
        int Y = 0;
        int Z = 0;
        double NetSignal = 0;
        double NetSignal2 = 0;
        List<double> TempList = new List<double>();
        double SoftmaxTotal = 0;
        double TempVal = 0;
        List<double> TempList2 = new List<double>();

        //Inputs = NormalizeDataSet(Inputs);

        //Set input layer neuron values to the input data values
        Y = 0;
        while (Y < Inputs.Count)
        {
            if(Double.IsNaN(Inputs[Y]) || Inputs[Y] < -12610000)
            {
                break;
            }
            TempList.Add(Inputs[Y]);


            Y++;
        }

        TempList = NormalizeDataSet(TempList,NormalizationMethod);

        Y = 0;
        while(Y < TempList.Count)
        {
            this.Layers[0].Neurons[Y].Value = TempList[Y];

            Y++;
        }
        //NormalizeLayerValues(this.Layers[0],1);

        while (X < this.Layers.Length)
        {
            TempList = new List<double>();
            NetSignal2 = 0;
            Y = 0;
            while (Y < this.Layers[X].Neurons.Length)
            {
                Z = 0;
                NetSignal = 0;

                //Sum up the net input signal into the current neuron
                while (Z < this.Layers[X].Neurons[Y].Dendrites.Length)
                {
                    NetSignal += this.Layers[X].Neurons[Y].Dendrites[Z].Weight * this.Layers[X - 1].Neurons[Z].Value;

                    if(Double.IsNaN(NetSignal))
                    {
                        break;
                    }

                    Z++;
                }

                //#HERE
                if(X < this.Layers.Length - 1)
                {
                    //NetSignal += (this.Layers[X].BiasNeuron.Value * this.Layers[X].BiasNeuron.Weight);
                }


                TempList.Add(NetSignal);
                //this.Layers[X].Neurons[Y].NetInput = NetSignal;
                //NetSignal = this.ActivationFunction(this.Layers[X].Neurons[Y], this.ActivationFunctionID);
                //this.Layers[X].Neurons[Y].Value = NetSignal;
                //TempList.Add(NetSignal);

                Y++;
            }

            TempList2 = NormalizeDataSet(TempList, NormalizationMethod);

            Y = 0;
            while (Y < this.Layers[X].Neurons.Length)
            {
                if (X < this.Layers.Length - 1)
                {
                    this.Layers[X].Neurons[Y].NetInput = TempList2[Y];
                    this.Layers[X].Neurons[Y].Value = this.ActivationFunction(this.Layers[X].Neurons[Y],ActivationFunctionIDs[X]);
                }
                else
                {
                    this.Layers[X].Neurons[Y].NetInput = TempList[Y];
                    this.Layers[X].Neurons[Y].Value = this.ActivationFunction(this.Layers[X].Neurons[Y], ActivationFunctionIDs[X]);
                }


                if (Math.Abs(this.Layers[X].Neurons[Y].NetInput) > 1000 || Math.Abs(this.Layers[X].Neurons[Y].Value) > 1000) 
                {
                    //break;
                }

                Y++;
            }

            X++;
        }
    }

    public List<double> ZeroCenterData(List<double> Inputs)
    {
        double DataMean = 0.0f;
        int X = 0;

        while(X < Inputs.Count)
        {
            DataMean += Inputs[X];

            X++;
        }

        DataMean = DataMean / Inputs.Count;

        X = 0;
        while(X < Inputs.Count)
        {
            Inputs[X] -= DataMean;

            X++;
        }

        return Inputs;
    }

    public List<double> NormalizeDataSet(List<double> Inputs,int NormalizationMethod = 0)
    {
        int X = 0;
        int Y = 0;
        double TempVal = 0;
        double StdDev = 0;
        double TotalVal = 0;
        List<double> TempList = new List<double>();
        double MinVal = 0;
        double MaxVal = 0;
        int MinibatchSize = 100;
        List<double> Outputs = new List<double>();

        switch (NormalizationMethod)
        {
            case 0:
                while(Y < Inputs.Count)
                {
                    X = 0;
                    TempVal = 0;
                    TempList = new List<double>();
                    while (X < MinibatchSize)
                    {
                        if(Y+X >= Inputs.Count)
                        {
                            break;
                        }    
                        TempList.Add((Inputs[Y+X]));
                        TempVal += (Inputs[Y+X]);

                        X++;
                    }

                    TempVal /= TempList.Count;
                    StdDev = CalculateStandardDeviation(TempList, TempVal);



                    X = 0;
                    while (X < TempList.Count)
                    {
                        if(StdDev == 0)
                        {
                            Outputs.Add(0);
                        }
                        else
                        {
                            Outputs.Add((TempList[X] - TempVal) / StdDev);
                        }                        

                        X++;
                    }

                    Y += MinibatchSize;
                }

                
                
                break;
            case 1:
                X = 0;
                MinVal = Inputs[0];
                MaxVal = Inputs[0];
                while (X < Inputs.Count)
                {
                    if (Inputs[X] < MinVal)
                    {
                        MinVal = Inputs[X];
                    }

                    if (Inputs[X] > MaxVal)
                    {
                        MaxVal = Inputs[X];
                    }

                    X++;
                }

                X = 0;
                while (X < Inputs.Count)
                {
                    Outputs.Add((Inputs[X] - MinVal) / (MaxVal - MinVal));

                    X++;
                }
                break;
            case 2:
                TempVal = 0;
                X = 0;
                while (X < Inputs.Count)
                {
                    TempVal += Inputs[X];

                    X++;
                }

                X = 0;
                while (X < Inputs.Count)
                {
                    Outputs.Add(Inputs[X] / TempVal);

                    X++;
                }
                break;
        }

        return Outputs;
    }

    public double CalculateMean(List<double> Inputs)
    {
        double TempVal = 0;
        int X = 0;

        while(X < Inputs.Count)
        {
            TempVal += Inputs[X];

            X++;
        }

        return TempVal / Inputs.Count;
    }



    public void NormalizeLayerWeights(Layer Input)
    {
        int X = 0;
        int Y = 0;
        int Z = 0;
        double TempVal = 0;
        double StdDev = 0;
        double TotalVal = 0;
        List<double> TempList = new List<double>();

        while (X < Input.Neurons.Length)
        {
            TempList = new List<double>();
            Y = 0;
            while(Y < Input.Neurons[X].Dendrites.Length)
            {
                TempList.Add(Input.Neurons[X].Dendrites[Y].Weight);
                TempVal += Input.Neurons[X].Dendrites[Y].Weight;

                Y++;
            }

            TempList = NormalizeDataSet(TempList, NormalizationMethod);

            Y = 0;
            while (Y < Input.Neurons[X].Dendrites.Length)
            {
               if(double.IsNaN(TempList[Y]))
                {
                    break;
                }

               Input.Neurons[X].Dendrites[Y].Weight = TempList[Y];

                Y++;
            }

            X++;
        }
    }

    public void NormalizeLayerValues(Layer Input,int NormalizationMethod = 2)
    {
        int X = 0;
        int Y = 0;
        double TempVal = 0;
        double StdDev = 0;
        double TotalVal = 0;
        List<double> TempList = new List<double>();
        double MinVal = 0;
        double MaxVal = 0;

        switch(NormalizationMethod)
        {
            case 0:
                while (X < Input.Neurons.Length)
                {
                    TempList.Add((Input.Neurons[X].Value));
                    TempVal += (Input.Neurons[X].Value);

                    X++;
                }

                TempVal /= Input.Neurons.Length;
                StdDev = CalculateStandardDeviation(TempList, TempVal);

                X = 0;
                while (X < Input.Neurons.Length)
                {
                    Input.Neurons[X].Value = (Input.Neurons[X].Value - TempVal) / StdDev;

                    if (Double.IsNaN(Input.Neurons[X].Value))
                    {
                        break;
                    }

                    X++;
                }
                break;
            case 1:
                X = 0;
                MinVal = Input.Neurons[0].Value;
                MaxVal = Input.Neurons[0].Value;
                while(X < Input.Neurons.Length)
                {
                    if(Input.Neurons[X].Value < MinVal)
                    {
                        MinVal = Input.Neurons[X].Value;
                    }

                    if (Input.Neurons[X].Value > MaxVal)
                    {
                        MaxVal = Input.Neurons[X].Value;
                    }

                    X++;
                }

                X = 0;
                while(X < Input.Neurons.Length)
                {
                    Input.Neurons[X].Value = (Input.Neurons[X].Value - MinVal) / (MaxVal - MinVal);

                    if(Input.Neurons[X].Value > 1000)
                    {
                        break;
                    }

                    X++;
                }    
                break;

            case 2:
                TempVal = 0;
                X = 0;
                while(X < Input.Neurons.Length)
                {
                    TempVal += Input.Neurons[X].Value;

                    X++;
                }

                X = 0;
                while(X < Input.Neurons.Length)
                {
                    Input.Neurons[X].Value /= TempVal;

                    X++;
                }
                break;
        }

    }

    public void UpdateLearningRate(int Epoch)
    {
        this.LearningRate *= (1.0f / (1.0f + (this.LearningRate / Epoch) * Epoch));
    }

    //Uses MSE (Mean Squared Error)
    private double CalculateNetworkError()
    {
        int X = 0;
        double TempVal = 0;

        while (X < this.Layers[this.Layers.Length - 1].Neurons.Length)
        {
            TempVal += (double)this.Layers[this.Layers.Length - 1].Neurons[X].Error;

            X++;
        }

        TempVal = (TempVal / this.Layers[this.Layers.Length - 1].Neurons.Length);
        TempVal *= TempVal;

        return TempVal;
    }

    public double CostFunction(Neuron InputVal,List<double> Actuals)
    {
        int X = 0;
        int PositiveActivation = 0;
        double TempVal = 0;

        while(PositiveActivation < this.Layers[this.Layers.Length - 1].Neurons.Length)
        {
            if(Actuals[PositiveActivation] == 1)
            {
                break;
            }

            PositiveActivation++;
        }

        TempVal = -1.0f * this.Layers[this.Layers.Length - 1].Neurons[PositiveActivation].NetInput + Math.Log(Math.Pow(Math.E, InputVal.NetInput));

        return TempVal;
    }

  
    public void BackPropagate2(List<double> Inputs, List<double> Actuals,List<int> ActivationFunctionIDs,List<int> LossFunctionIDs,bool NormalizeOutput = false)
    {
        int X = 0;
        int Y = 0;
        int Z = 0;
        double TempVal = 0;
        double TempVal2 = 0;
        double TempVal3 = 0;
        double TempVal4 = 0;
        List<double> Errors = new List<double>();
        List<List<List<double>>> NewWeights = new List<List<List<double>>>();
        List<double> TempWeights = new List<double>();
        List<List<double>> TempWeights2 = new List<List<double>>();

        //To have an error value to mitigate, we must first generate output for the network
        this.FeedForward(Inputs,ActivationFunctionIDs,NormalizeOutput);

        X = this.Layers.Length - 1;
        this.Layers[X].TotalLayerError = 0;
        TempWeights2 = new List<List<double>>();

        //Calculate error for output layer
        Y = 0;
        while (Y < this.Layers[X].Neurons.Length)
        {
            //Actual error value of the output neuron
            this.Layers[X].Neurons[Y].Error = this.Layers[X].Neurons[Y].Value - Actuals[Y];
            //this.Layers[X].Neurons[Y].Error = (this.Layers[X].Neurons[Y].Value - Actuals[Y]) * (this.Layers[X].Neurons[Y].Value - Actuals[Y]) / (this.Layers[X].Neurons.Length);

            TempWeights = new List<double>();
            Z = 0;
            while (Z < this.Layers[X].Neurons[Y].Dendrites.Length)
            {
                TempVal = (double)this.ActivationFunctionDerivative(this.Layers[X].Neurons[Y], ActivationFunctionIDs[X]);
                TempVal2 = (double)this.Layers[X].Neurons[Y].Error * (double)this.ActivationFunctionDerivative(this.Layers[X].Neurons[Y], ActivationFunctionIDs[X]) * (this.Layers[X - 1].Neurons[Z].Value);
                TempVal3 = (this.Layers[X].Neurons[Y].Dendrites[Z].Weight - TempVal2 * (double)this.LearningRate);

                if(double.IsNaN(TempVal3))
                {
                    break;
                }

                TempWeights.Add(this.Layers[X].Neurons[Y].Dendrites[Z].Weight - TempVal2 * (double)this.LearningRate * this.Momentum);

                Z++;
            }

            TempWeights2.Add(TempWeights);

            Y++;
        }

        NewWeights.Add(TempWeights2);

        //Propagate error through hidden layers
        X = this.Layers.Length - 2;
        while (X >= 0)
        {
            TempWeights2 = new List<List<double>>();
            Y = 0;
            while (Y < this.Layers[X].Neurons.Length)
            {

                TempWeights = new List<double>();
                TempVal2 = 0;
                Z = 0;
                while (Z < this.Layers[X + 1].Neurons.Length)
                {
                    //TempVal = (double)(this.Layers[X + 1].Neurons[Z].Error) * (double)this.ActivationFunctionDerivative(this.Layers[X + 1].Neurons[Z], ActivationFunctionIDs[X]) * this.Layers[X + 1].Neurons[Z].Dendrites[Y].Weight;
                    TempVal = (double)this.ActivationFunctionDerivative(this.Layers[X + 1].Neurons[Z], ActivationFunctionIDs[X]) * this.Layers[X + 1].Neurons[Z].Dendrites[Y].Weight;
                    TempVal2 += TempVal;                    

                    Z++;
                }

                TempVal2 = TempVal2 / (double)this.Layers[X + 1].Neurons.Length;
                this.Layers[X].Neurons[Y].Error = TempVal2;

                if (Double.IsNaN(TempVal2))
                {
                    break;
                }               

                TempVal3 = 0;
                Z = 0;
                while (Z < this.Layers[X].Neurons[Y].Dendrites.Length)
                {
                    if (X == 0)
                    {
                        TempVal3 = (double)TempVal2 * (double)this.ActivationFunctionDerivative(this.Layers[X].Neurons[Y], ActivationFunctionIDs[X]) * (Inputs[Z]);
                    }
                    else
                    {
                        TempVal3 = (double)TempVal2 * (double)this.ActivationFunctionDerivative(this.Layers[X].Neurons[Y], ActivationFunctionIDs[X]) * (this.Layers[X - 1].Neurons[Z].Value);
                    }

                    TempVal4 = this.Layers[X].Neurons[Y].Dendrites[Z].Weight - TempVal3 * (double)this.LearningRate;

                    if(double.IsNaN(TempVal4))
                    {
                        break;
                    }

                    TempWeights.Add(this.Layers[X].Neurons[Y].Dendrites[Z].Weight - TempVal3 * (double)this.LearningRate * this.Momentum);

                    Z++;
                }

                TempWeights2.Add(TempWeights);

                Y++;
            }

            NewWeights.Add(TempWeights2);

            X--;
        }

        //Batch weight update
        X = 0;
        while (X < NewWeights.Count)
        {
            Y = 0;
            while (Y < NewWeights[X].Count)
            {
                Z = 0;
                while (Z < NewWeights[X][Y].Count)
                {
                    this.Layers[this.Layers.Length - 1 - X].Neurons[Y].Dendrites[Z].Weight = NewWeights[X][Y][Z];

                    Z++;
                }

                Y++;
            }

            if(X < this.Layers.Length - 1)
            {
                NormalizeLayerWeights(this.Layers[X]);
            }
            
            

            X++;
        }
    }

    public double LossFunction(double InputVal, double ExpectedVal,int LayerID, int FunctionID)
    {
        double Output = 0;
        int X;

        switch(FunctionID)
        {
            case 0:
                Output = (InputVal - ExpectedVal) * (InputVal - ExpectedVal) / this.Layers[LayerID].Neurons.Length;

                break;
            case 1:
                X = 0;

                Output = 1 - (ExpectedVal * InputVal);
                break;
        }


        return Output;
    }

    public double CalculateStandardDeviation(List<double> Inputs,double Mean = 0.0f)
    {
        double Variance = 0;
        int X = 0;

        if(Mean == 0)
        {
            while (X < Inputs.Count)
            {
                Mean += Inputs[X];

                X++;
            }

            Mean /= Inputs.Count;
        }

        X = 0;
        while(X < Inputs.Count)
        {
            Variance += Math.Pow(Inputs[X] - Mean,2);

            X++;
        }

        Variance /= Inputs.Count;

        return Math.Sqrt(Variance);
    }

    public double ZScore(List<double> Inputs,double SamplePoint,double Mean = 0.0f)
    {
        int X = 0;

        if(Mean == 0)
        {
            while (X < Inputs.Count)
            {
                Mean += Inputs[X];

                X++;
            }

            Mean /= Inputs.Count;
        }

        return (SamplePoint - Mean) / CalculateStandardDeviation(Inputs);
    }

    public void BackPropagateError(List<double> Inputs)
    {
        int X = 0;
        int Y = 0;
        int Z = 0;
        double TempVal = 0;
        double TempVal2 = 0;
        double TempVal3 = 0;
        double TempVal4 = 0;
        List<double> Errors = new List<double>();
        List<List<List<double>>> NewWeights = new List<List<List<double>>>();
        List<double> TempWeights = new List<double>();
        List<List<double>> TempWeights2 = new List<List<double>>();
        double StdDev = 0;

        //To have an error value to mitigate, we must first generate output for the network

        //this.FeedForward(Inputs);

        //Inputs = NormalizeDataSet(Inputs,1);

        X = this.Layers.Length - 1;
        this.Layers[X].TotalLayerError = 0;
        TempWeights2 = new List<List<double>>();

        //Calculate error for output layer
        Y = 0;
        while (Y < this.Layers[X].Neurons.Length)
        {
            //TempVal = (0 * Math.Log(Inputs[Y]) + (1 - 0) * Math.Log(1 - Inputs[Y])) ;
            //TempVal = (Inputs[Y] * Inputs[Y]) / this.Layers[X].Neurons.Length;
            TempVal = (Inputs[Y]);

            this.Layers[X].Neurons[Y].Error = (TempVal);

            TempWeights = new List<double>();
            Z = 0;
            while (Z < this.Layers[X].Neurons[Y].Dendrites.Length)
            {
                TempVal2 = this.Layers[X].Neurons[Y].Error;
                //TempVal2 = this.Layers[X - 1].Neurons[Z].Value * (this.Layers[X].Neurons[Y].Value - Actuals[Y]);

                //TempVal = this.Layers[X - 1].Neurons[Z].Value * (this.Layers[X].Neurons[Y].Softmax - Actuals[Y]);
                //TempVal2 = Inputs[Y] * (double)this.ActivationFunctionDerivative(this.Layers[X].Neurons[Y], 4,Actuals[Y]) * (this.Layers[X - 1].Neurons[Z].Value);
                
                //TempVal2 = (double)(TempVal) * (double)this.ActivationFunctionDerivative(this.Layers[X].Neurons[Y], ActivationFunctionID) * (this.Layers[X - 1].Neurons[Z].Value);
                
                //TempVal3 = (this.Layers[X].Neurons[Y].Dendrites[Z].Weight - TempVal2 * (double)this.LearningRate);
                //TempVal2 = this.Layers[X].Neurons[Y].Dendrites[Z].Weight - TempVal2 * this.LearningRate * this.Momentum;

                //TempWeights.Add(Inputs[Y]);
                TempVal = this.Layers[X].Neurons[Y].Dendrites[Z].Weight - (TempVal2) * this.LearningRate * this.Momentum;

                if(double.IsNaN(TempVal))
                {
                    break;
                }

                TempWeights.Add(TempVal);

                Z++;
            }

            TempWeights2.Add(TempWeights);

            Y++;
        }

        NewWeights.Add(TempWeights2);

        //Propagate error through hidden layers
        X = this.Layers.Length - 2;
        while (X > 0)
        {
            TempWeights2 = new List<List<double>>();
            Y = 0;
            while (Y < this.Layers[X].Neurons.Length)
            {

                TempWeights = new List<double>();
                TempVal2 = 0;
                Z = 0;
                while (Z < this.Layers[X + 1].Neurons.Length)
                {
                    TempVal = (double)(this.Layers[X + 1].Neurons[Z].Error) * (double)this.ActivationFunctionDerivative(this.Layers[X + 1].Neurons[Z], this.ActivationFunctionID) * this.Layers[X + 1].Neurons[Z].Dendrites[Y].Weight;

                    if (Double.IsInfinity(TempVal))
                    {
                        break;
                    }

                    TempVal2 += TempVal;

                    Z++;
                }

                if (Double.IsNaN(TempVal2))
                {
                    //TempVal2 = 1;
                    break;
                }

                this.Layers[X].Neurons[Y].Error = TempVal2;

                TempVal3 = 0;
                Z = 0;
                while (Z < this.Layers[X].Neurons[Y].Dendrites.Length)
                {
                    if (X == 0)
                    {
                        TempVal3 = (double)TempVal2 * (double)this.ActivationFunctionDerivative(this.Layers[X].Neurons[Y], this.ActivationFunctionID) * (Inputs[Z]);
                    }
                    else
                    {
                        TempVal3 = (double)TempVal2 * (double)this.ActivationFunctionDerivative(this.Layers[X].Neurons[Y], this.ActivationFunctionID) * (this.Layers[X - 1].Neurons[Z].Value);
                    }

                    if (Double.IsNaN(TempVal3))
                    {
                        break;
                    }

                    TempVal4 = this.Layers[X].Neurons[Y].Dendrites[Z].Weight - TempVal3 * (double)this.LearningRate;

                    if(double.IsNaN(TempVal4))
                    {
                        break;
                    }

                    //Bias update
                    //this.Layers[X].BiasNeuron.Weight -= this.LearningRate * TempVal3;

                    //Weight clipping
                    if ((this.Layers[X].BiasNeuron.Weight) > GradientClipFactor)
                    {
                        this.Layers[X].BiasNeuron.Weight = GradientClipFactor;
                    }
                    else if (this.Layers[X].BiasNeuron.Weight < -1 * GradientClipFactor)
                    {
                        this.Layers[X].BiasNeuron.Weight = -1 * GradientClipFactor;
                    }

                    TempWeights.Add(this.Layers[X].Neurons[Y].Dendrites[Z].Weight - TempVal3 * (double)this.LearningRate * this.Momentum);

                    Z++;
                }

                TempWeights2.Add(TempWeights);

                Y++;
            }

            NewWeights.Add(TempWeights2);

            X--;
        }

        //Batch weight update
        X = 0;
        while (X < NewWeights.Count)
        {
            Y = 0;
            while (Y < NewWeights[X].Count)
            {
                Z = 0;
                while (Z < NewWeights[X][Y].Count)
                {
                    if (Double.IsNaN(NewWeights[X][Y][Z]))
                    {
                        break;
                    }

                    this.Layers[this.Layers.Length - 1 - X].Neurons[Y].Dendrites[Z].Weight = NewWeights[X][Y][Z];

                    //Weight clipping
                    if ((NewWeights[X][Y][Z]) > GradientClipFactor)
                    {
                        this.Layers[this.Layers.Length - 1 - X].Neurons[Y].Dendrites[Z].Weight = GradientClipFactor;
                    }
                    else if (NewWeights[X][Y][Z] < -1 * GradientClipFactor)
                    {
                        this.Layers[this.Layers.Length - 1 - X].Neurons[Y].Dendrites[Z].Weight = -1 * GradientClipFactor;
                    }


                    Z++;
                }

                Y++;
            }

            if(X < this.Layers.Length - 1)
            {
                NormalizeLayerWeights(this.Layers[X]);
            }
            
            

            X++;
        }
    }

    //Loads an existing network configuration from file
    public NeuralNetwork LoadNetwork(string FilePath)
    {
        NeuralNetwork NewANN = new NeuralNetwork();
        Regex Regx;
        Regex Regx2;
        Regex Regx3;
        MatchCollection TempCol;
        MatchCollection TempCol2;
        MatchCollection TempCol3;
        int X = 0;
        int Y = 0;
        int Z = 0;
        string FileStr = "";
        string TempStr = "";
        List<Dendrite> TempDendrites;
        List<Neuron> TempNeurons;
        List<Layer> TempLayers;
        Neuron TempNeuron;
        Layer TempLayer;

        FileStr = System.IO.File.ReadAllText(FilePath);

        TempLayers = new List<Layer>();
        Regx = new Regex("(?:\\<LAYER\\>)([\n\\s\\S]*?)(?:\\<\\/LAYER\\>)");
        TempCol = Regx.Matches(FileStr);
        X = 0;

        while (X < TempCol.Count)
        {
            TempLayer = new Layer(X);
            TempNeurons = new List<Neuron>();
            Regx2 = new Regex("(?:\\<NEURON\\>)([\n\\s\\S]*?)(?:\\<\\/NEURON\\>)");
            TempCol2 = Regx2.Matches(TempCol[X].Value);
            Y = 0;

            while (Y < TempCol2.Count)
            {
                TempNeuron = new Neuron();
                TempDendrites = new List<Dendrite>();
                Regx3 = new Regex("(?:\\<DENDRITE\\>)([\n\\s\\S]*?)(?:\\<\\/DENDRITE\\>)");
                TempCol3 = Regx3.Matches(TempCol2[Y].Value);
                Z = 0;

                while (Z < TempCol3.Count)
                {
                    TempDendrites.Add(new Dendrite((double)Double.Parse(TempCol3[Z].Value.ToString().Replace("DENDRITE", "").Replace("<", "").Replace("/", "").Replace(">", ""))));

                    Z++;
                }

                TempNeuron.Dendrites = TempDendrites.ToArray();

                Regx3 = new Regex("(?:\\<ERROR\\>)([\n\\s\\S]*?)(?:\\<\\/ERROR\\>)");
                TempCol3 = Regx3.Matches(TempCol2[Y].Value);
                TempNeuron.Error = (double)Double.Parse(TempCol3[0].Value.ToString().Replace("ERROR", "").Replace("<", "").Replace("/", "").Replace(">", ""));

                //Regx3 = new Regex("(?:\\<BIAS\\>)([\n\\s\\S]*?)(?:\\<\\/BIAS\\>)");
                //TempCol3 = Regx3.Matches(TempCol2[Y].Value);
                //TempLayer.BiasNeuron.Weight = (double)Double.Parse(TempCol3[0].Value.ToString().Replace("BIAS", "").Replace("<", "").Replace("/", "").Replace(">", ""));

                Regx3 = new Regex("(?:\\<VALUE\\>)([\n\\s\\S]*?)(?:\\<\\/VALUE\\>)");
                TempCol3 = Regx3.Matches(TempCol2[Y].Value);
                TempNeuron.Value = (double)Double.Parse(TempCol3[0].Value.ToString().Replace("VALUE", "").Replace("<", "").Replace("/", "").Replace(">", ""));

                TempNeurons.Add(TempNeuron);

                Y++;
            }

            TempLayer.Neurons = TempNeurons.ToArray();
            TempLayers.Add(TempLayer);

            X++;
        }

        NewANN.Layers = TempLayers.ToArray();

        return NewANN;
    }

    //Save current network configuration (once it has been fully trained)
    public void SaveNetwork(string FilePath)
    {
        string TempStr = "";
        int X = 0;
        int Y = 0;
        int Z = 0;
        StringBuilder StringBldr = new StringBuilder();

        StringBldr.Append("<LEARNRATE>" + this.LearningRate + "</LEARNRATE>\n");

        while (X < this.Layers.Length)
        {
            StringBldr.Append("<LAYER>\n\t<ID>" + X.ToString() + "</ID>\n");
            Y = 0;

            while (Y < this.Layers[X].Neurons.Length)
            {
                StringBldr.Append("\t<NEURON>\n\t\t<ERROR>" + this.Layers[X].Neurons[Y].Error.ToString("F15") + "</ERROR>\n\t\t<BIAS>" + this.Layers[X].BiasNeuron.Weight.ToString("F15") + "</BIAS>\n\t\t<VALUE>" + this.Layers[X].Neurons[Y].Value.ToString("F15") + "</VALUE>\n");
                Z = 0;

                while (Z < this.Layers[X].Neurons[Y].Dendrites.Length)
                {
                    StringBldr.Append("\t\t<DENDRITE>" + this.Layers[X].Neurons[Y].Dendrites[Z].Weight.ToString("F15") + "</DENDRITE>\n");

                    Z++;
                }

                StringBldr.Append("\t</NEURON>\n");

                Y++;
            }

            StringBldr.Append("</LAYER>\n");

            X++;
        }

        System.IO.File.WriteAllText(FilePath, StringBldr.ToString());
    }

    public bool Dropout(double DropoutLimit)
    {
        int X = 0;
        double TempVal = 0;

        while(X < this.Layers[this.Layers.Length - 1].Neurons.Length)
        {
            TempVal += this.Layers[this.Layers.Length - 1].Neurons[X].Error;

            X++;
        }

        TempVal = TempVal / this.Layers[this.Layers.Length - 1].Neurons.Length;

        if(TempVal < DropoutLimit && TempVal > 0)
        {
            return true;
        }
        else
        {
            return false;
        }
    }


    //Highly recommend using ReLu, much newer and more efficient
    public double ActivationFunction(Neuron InputVal, int FunctionID)
    {
        double TempVal = 0;
        double TempVal2 = 0;
        double TempVal3 = 0;
        int X = 0;

        //Leaky ReLu requires a specific small constant for optimization, can be anything < 1 and > 0

        //Runs the activation function that correlates to the ActivationFunctionID network parameter
        switch (FunctionID)
        {
            //TanH
            case 1:
                TempVal = (double)Math.Tanh(InputVal.NetInput);

                if(Double.IsInfinity(TempVal))
                {
                    //TempVal = 1;
                }

                if(TempVal == 1 )
                {
                   // break;
                }

                break;
            //Logistic Sigmoid
            case 2:

                TempVal2 = Math.Exp(InputVal.NetInput);
                //TempVal2 =  TempVal2 * 0.00001;

                TempVal = TempVal2 / (TempVal2 + 1);

                if(Double.IsNaN(TempVal))
                {
                    TempVal = 1;
                }

                //TempVal = (double)(1.0f / (1.0f + (Math.Pow((double)Math.E, -1.0f * (InputVal.NetInput)))));
                break;
            //ELU
            case 3:
                if(InputVal.NetInput >= 0)
                {
                    TempVal = InputVal.NetInput;
                }
                else
                {
                    TempVal = ReLUConst * (Math.Exp(InputVal.NetInput) - 1);
                }
                if(Double.IsNaN(TempVal))
                {
                    //TempVal = 1;
                    break;
                }

                if(TempVal == 0)
                {
                    //break;
                }

                break;
            case 4:
                //ReLU
                if (InputVal.NetInput < 0)
                {
                    TempVal = ReLUConst * InputVal.NetInput;
                }
                else
                {
                    TempVal = InputVal.NetInput;
                }
                
                

                break;
        }

        return TempVal;
    }

    public double ActivationFunctionDerivative(Neuron InputVal, int FunctionID, double Actual = 0)
    {
        double TempVal = 0;
        int X = 0;
        int Y = 0;

        //Leaky ReLu requires a specific small constant for optimization, can be anything < 1 and > 0

        //Runs the activation function DERIVATIVE that correlates to the ActivationFunctionID network parameter
        switch (FunctionID)
        {
            //TanH
            case 1:
                TempVal = (double)(1 - (Math.Tanh(InputVal.NetInput) * Math.Tanh(InputVal.NetInput)));
                if (TempVal == 0 || TempVal > 255)
                {
                    //break;
                }
                break;
            //Logistic Sigmoid
            case 2:
                TempVal = this.ActivationFunction(InputVal, 2) * (1 - this.ActivationFunction(InputVal, 2));
                break;
            //Leaky ReLu (Regressive Linear Unit)
            case 3:
                TempVal = this.ActivationFunction(InputVal, 3) + ReLUConst;

                if (Double.IsNaN(TempVal))
                {
                    break;
                }

                break;
            //Softmax
            case 4:
                if (InputVal.NetInput >= 0)
                {
                    TempVal = 1;
                }
                else
                {
                    TempVal = ReLUConst;
                }



                break;
        }

        return TempVal;
    }

}
